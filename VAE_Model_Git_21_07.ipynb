{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robinmaeschreiner/VAE_Model/blob/main/VAE_Model_Git_21_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVy68Z9QfyoN"
      },
      "source": [
        "## Inspirations and Links to Useful Projects:\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/variational-autoencoders-vaes-for-dummies-step-by-step-tutorial-69e6d1c9d8e9\n",
        "https://colab.research.google.com/github/lschmiddey/fastpages_/blob/master/_notebooks/2021-03-14-tabular-data-variational-autoencoder.ipynb#scrollTo=ZG91fCG40vWz \n",
        "\n",
        "https://github.com/lschmiddey/fastpages_/blob/master/_notebooks/2021-03-14-tabular-data-variational-autoencoder.ipynb \n",
        "\n",
        "https://github.com/lschmiddey - lschmiddey\n",
        "\n",
        "https://gitlab.com/m4gpie/self-supervised-ecoacoustics/-/blob/main/code/ecoacoustics/models/encoder.py \n",
        "\n",
        "\n",
        "https://github.com/geyang/variational_autoencoder_pytorch/blob/master/model.py "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVHeCupogti9"
      },
      "source": [
        "## Setup and Prerequisites "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP4C7ry7ft_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424d879d-6185-4f90-b63a-a6b061704b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: feature-engine in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.21.6)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (0.13.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature-engine) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature-engine) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.3->feature-engine) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->feature-engine) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->feature-engine) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.1->feature-engine) (0.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.1->feature-engine) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->statsmodels>=0.11.1->feature-engine) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "! pip install feature-engine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nal-iyuL6S_f",
        "outputId": "1c5bff32-a639-43a5-bc64-175581a3d4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.21)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.7.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY7-iqalgGm0"
      },
      "outputs": [],
      "source": [
        "#Import Statements \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from numpy import asarray\n",
        "from feature_engine.imputation import RandomSampleImputer\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.modules.activation import LeakyReLU\n",
        "from torch.nn.modules.activation import Sigmoid\n",
        "from torch.nn.modules.activation import ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW0SO35xhgR9"
      },
      "source": [
        "# Functions and Theory:\n",
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYQ76Xwyjyf2"
      },
      "outputs": [],
      "source": [
        "#using the RandomSampleImputer library to impute mixed data sets - categorical and numerical\n",
        "def impute_mixed_data(dataset_mixed): \n",
        "  # set up the imputer\n",
        "  rs_imputer = RandomSampleImputer()\n",
        "\n",
        "  # fit the imputer\n",
        "  dataset_mixed_imputed = rs_imputer.fit_transform(dataset_mixed)\n",
        "  return dataset_mixed_imputed "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8lUvXOZkeeX"
      },
      "outputs": [],
      "source": [
        "# a function to use ordinal encoding for the columsn in the data that have continous categorical data following a natural order, this order can be specified if known but needs [] around it as originally the OrdinalEncoder function expects a list of columns \n",
        "def ordinal_encod(df_columns, ordered_catg='auto'): \n",
        "\n",
        "  data_col = asarray(df_columns)\n",
        "  data_col = data_col.reshape(-1, 1)\n",
        "\n",
        "  # define and transform data with ordinal encoding\n",
        "  encoder_ordinal = OrdinalEncoder(categories=ordered_catg) # \n",
        "  encoder_ordinal.fit(data_col)\n",
        "  #print(encoder_ordinal.categories_)\n",
        "  ord_enc_col = encoder_ordinal.transform(data_col)\n",
        "\n",
        "  #visualization\n",
        "  df_ord_enc_col = pd.DataFrame(ord_enc_col )\n",
        "  #display(df_ord_enc_col.iloc[:7])\n",
        "\n",
        "  return ord_enc_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElmR8oIMhqPd"
      },
      "outputs": [],
      "source": [
        "def load_and_standardize_data(df):\n",
        "\n",
        "    # replace nan with mean \n",
        "    df = np.array(df)\n",
        "    df = df.reshape(-1, df.shape[1]).astype('float32')\n",
        "\n",
        "    # randomly split\n",
        "    X_train, X_test = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "    # standardize values\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)   \n",
        "    return X_train, X_test, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNPaRCbDmTOn"
      },
      "outputs": [],
      "source": [
        "# function to build rtain and test datasets for a given batch size using the DataBuilder Class below\n",
        "def test_train_datasets(df, batch_size): \n",
        "\n",
        "  traindata_set = DataBuilder(df, train=True) # drop first column which is the PID and PTID - personal ID as it cannot be predicted\n",
        "  testdata_set = DataBuilder(df, train=False)\n",
        "\n",
        "  trainloader = DataLoader(dataset=traindata_set,batch_size= batch_size) \n",
        "  testloader= DataLoader(dataset=testdata_set,batch_size= batch_size) \n",
        "\n",
        "  print('Data Shape:', trainloader.dataset.x.shape, testloader.dataset.x.shape)\n",
        "  return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2jfg_X3h1_u"
      },
      "outputs": [],
      "source": [
        "# Class to standardize, pre-process and build the train and test datasets for the VAE training\n",
        "class DataBuilder(Dataset):\n",
        "    def __init__(self, df, train=True):\n",
        "        self.X_train, self.X_test, self.standardizer  = load_and_standardize_data(df)\n",
        "        if train:\n",
        "            self.x = torch.from_numpy(self.X_train)\n",
        "            self.len=self.x.shape[0]\n",
        "        else:\n",
        "            self.x = torch.from_numpy(self.X_test)\n",
        "            self.len=self.x.shape[0]\n",
        "        del self.X_train\n",
        "        del self.X_test \n",
        "    def __getitem__(self,index):      \n",
        "        return self.x[index]\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HdAv5RIiJsc"
      },
      "source": [
        "## VAE Model "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self,D_in,H,H2,latent_dim, activation=nn.ReLU(), inbetween_layer=nn.BatchNorm1d):\n",
        "        \n",
        "        #Encoder\n",
        "        super(Autoencoder,self).__init__()\n",
        "        self.linear1=nn.Linear(D_in,H)\n",
        "        self.lin_additional1 = inbetween_layer(num_features=H)\n",
        "        self.linear2=nn.Linear(H,H2) # adda relu layer for non-linearity\n",
        "        self.lin_additional2 = inbetween_layer(num_features=H2)\n",
        "        self.linear3=nn.Linear(H2,H2)\n",
        "        self.lin_additional3 = inbetween_layer(num_features=H2)\n",
        "        \n",
        "        # Latent vectors mu and sigma\n",
        "        self.fc1 = nn.Linear(H2, latent_dim)\n",
        "        self.fc_additional1 =inbetween_layer(num_features=latent_dim)\n",
        "        self.fc21 = nn.Linear(latent_dim, latent_dim) #the output layer for the mean vector of size 7 for each latent dimension\n",
        "        self.fc22 = nn.Linear(latent_dim, latent_dim) #the output layer for the variance vector of size 7 for each latent dimension\n",
        "        # however, want a variance/sigma vector of size 37 one for each input vector! \n",
        "\n",
        "        # Sampling vector\n",
        "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
        "        self.fc_additional3 = inbetween_layer(latent_dim)\n",
        "        self.fc4 = nn.Linear(latent_dim, H2)\n",
        "        self.fc_additional4 = inbetween_layer(H2)\n",
        "        \n",
        "        # Decoder\n",
        "        self.linear4=nn.Linear(H2,H2)\n",
        "        self.lin_additional4 = inbetween_layer(num_features=H2)\n",
        "        self.linear5=nn.Linear(H2,H)\n",
        "        self.lin_additional5 = inbetween_layer(num_features=H)\n",
        "\n",
        "        self.linear6=nn.Linear(H,D_in)\n",
        "        self.linear61=nn.Linear(H,D_in) # the output vector of this layer will be the size of the input features to get the predicted mean\n",
        "        self.linear62=nn.Linear(H,D_in) # the output vector of this layer will be the size of the input features to get the predicted log_variance\n",
        "\n",
        "        # Mean and Variance from Reconstruction\n",
        "        self.fc51 = nn.Linear(D_in, D_in) #the output layer for the mean vector of size 7 for each latent dimension\n",
        "        self.fc52 = nn.Linear(D_in, D_in) #the output layer for the variance vector of size 7 for each latent dimension\n",
        "\n",
        "        # Activation Function\n",
        "        self.activation = activation  #change this to self.activation and add an additional parameter to the AutoEncoder class which represents the type of activation function that should be tested out\n",
        "        \n",
        "\n",
        "    def encode(self, x):\n",
        "        lin1 = self.activation(self.lin_additional1(self.linear1(x)))\n",
        "        lin2 = self.activation(self.lin_additional2(self.linear2(lin1)))\n",
        "        lin3 = self.activation(self.lin_additional3(self.linear3(lin2)))\n",
        "\n",
        "        fc1 = self.activation(self.fc_additional1(self.fc1(lin3)))\n",
        "\n",
        "        z_mu = self.fc21(fc1)\n",
        "        z_logvar = self.fc22(fc1)\n",
        "        \n",
        "        return z_mu, z_logvar\n",
        "    \n",
        "    def reparameterize(self, z_mu, z_logvar):                          \n",
        "        std = z_logvar.mul(0.5).exp_()\n",
        "        eps = torch.distributions.Normal(z_mu, std).rsample()\n",
        "        return eps.mul(std).add_(z_mu) #returns the latent vector z sampled from the mean and variance \n",
        "\n",
        "        \n",
        "    def decode(self, z):\n",
        "        fc3 = self.activation(self.fc_additional3(self.fc3(z)))\n",
        "        fc4 = self.activation(self.fc_additional4(self.fc4(fc3)))\n",
        "\n",
        "        lin4 = self.activation(self.lin_additional4(self.linear4(fc4)))\n",
        "        lin5 = self.activation(self.lin_additional5(self.linear5(lin4)))\n",
        "\n",
        "        #outputs\n",
        "        x_recon = self.linear6(lin5)\n",
        "        x_recon_mu = self.linear61(lin5) \n",
        "        x_recon_log_var = self.linear62(lin5)\n",
        "\n",
        "        return x_recon, x_recon_mu, x_recon_log_var\n",
        "\n",
        "\n",
        "    def forward(self, x): \n",
        "        z_mu, z_logvar = self.encode(x)\n",
        "        z = self.reparameterize(z_mu, z_logvar)\n",
        "        x_recon, x_recon_mu, x_recon_logvar = self.decode(z)\n",
        "\n",
        "        return x_recon, z_mu, z_logvar, x_recon_mu, x_recon_logvar\n",
        "\n",
        "\n",
        "    def test_just_meam(self, x): #instead of decoding the sampled z - only the mean is input -> bug testing\n",
        "        z_mu, z_logvar = self.encode(x)\n",
        "        x_recon, x_recon_mu, x_recon_logvar = self.decode(z_mu)\n",
        "\n",
        "        return x_recon, z_mu, z_logvar, x_recon_mu, x_recon_logvar"
      ],
      "metadata": {
        "id": "-ioTtSCXI-Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0w7oB1aiimM"
      },
      "source": [
        "Custom Loss Classes using MSE and or Individual Variances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TymgEKWgidUu"
      },
      "outputs": [],
      "source": [
        "class customLoss(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(customLoss, self).__init__()\n",
        "        self.mse_loss = nn.MSELoss(reduction=\"mean\")\n",
        "    \n",
        "    def forward(self, x_recon, x, mu, logvar, x_recon_logvar): #x_recon_logvar is not needed but included as a place holder so that the right number of values is unpacked when changing between the functions\n",
        "        loss_MSE = self.mse_loss(x_recon, x)\n",
        "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return loss_MSE + loss_KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTdhc45bxg1D"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.loss import GaussianNLLLoss\n",
        "#Inferring an individual variance for each feature - represent how confident the model is in predicting a given feature\n",
        "\n",
        "class customLoss_individual(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(customLoss_individual, self).__init__()\n",
        "        self.log_prob_Gaus_loss = nn.GaussianNLLLoss(reduction='none')\n",
        "    \n",
        "    \n",
        "    def forward(self, x_recon, x, z_mu, z_logvar, x_recon_logvar): \n",
        "        loss_log_prob_sum = torch.sum(self.log_prob_Gaus_loss(x_recon, x, torch.exp(x_recon_logvar)), axis=1) #take the sum over the observations -> sum up the 37 values to get a value for each sample in the batch of size 500\n",
        "        #print(loss_log_prob_sum.size())\n",
        "        loss_log_prob = loss_log_prob_sum.mean(axis=0) #take the batch-wise mean \n",
        "        #print('Batch-wise Mean Gaussian',loss_log_prob)\n",
        "\n",
        "        loss_KLD_sum =  -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp()).sum(axis=0)\n",
        "        loss_KLD = loss_KLD_sum.mean(axis=0) #after summing the KL terms for each of the latent space variables it takes the batch-wise mean\n",
        "\n",
        "        return loss_log_prob  + loss_KLD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class customLoss_Gaussian_NLLL(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(customLoss_Gaussian_NLLL, self).__init__()\n",
        "        self.log_prob_Gaus_loss = nn.GaussianNLLLoss(reduction='none') \n",
        "    \n",
        "    def forward(self, x_recon, x, z_mu, z_logvar, x_recon_logvar): \n",
        "        loss_log_prob_sum = torch.sum(self.log_prob_Gaus_loss(x_recon, x, torch.exp(x_recon_logvar)), axis=-1) #take the sum over the observations -> sum up the 37 values to get a value for each sample in the batch of size 500\n",
        "        loss_log_prob = loss_log_prob_sum.mean(axis=0) #take the batch-wise mean - mean over the rows \n",
        "\n",
        "        return loss_log_prob  "
      ],
      "metadata": {
        "id": "wdhzkbsoVdY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customLoss_KL_divergence(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(customLoss_KL_divergence, self).__init__()\n",
        "    \n",
        "    def forward(self, x_recon, x, z_mu, z_logvar, x_recon_logvar): \n",
        "        loss_KLD_sum =  -0.5 * torch.sum(1 + z_logvar - z_mu.pow(2) - z_logvar.exp()).sum(axis=-1) #taking the sum over the latent dimensions\n",
        "        loss_KLD = loss_KLD_sum.mean(axis=0) #after summing the KL terms taking the batch-wise mean\n",
        "\n",
        "        return loss_KLD"
      ],
      "metadata": {
        "id": "X1CKUayYBPRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keQvDYgyqLeH"
      },
      "source": [
        "## Training VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2B_jzHfqKwo"
      },
      "outputs": [],
      "source": [
        "def train(epoch, trainloader, model, optimizer, customized_loss, train_losses, return_loss=False):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_idx, data in enumerate(trainloader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor s to zero\n",
        "        x_recon, z_mu, z_logvar, x_recon_mu, x_recon_logvar = model(data)\n",
        "        loss = customized_loss(x_recon, data, z_mu, z_logvar, x_recon_logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:        \n",
        "        print('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
        "            epoch, train_loss / len(trainloader.dataset)))\n",
        "        \n",
        "    train_losses.append(train_loss / len(trainloader.dataset))\n",
        "\n",
        "    if return_loss==True :\n",
        "      return train_loss / len(trainloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWvxBLgYq8bo"
      },
      "outputs": [],
      "source": [
        "def test(epoch, testloader, model, optimizer, customized_loss, test_losses):\n",
        "\n",
        "    with torch.no_grad(): #The wrapper with torch.no_grad() temporarily sets all of the requires_grad flags to false\n",
        "        test_loss = 0\n",
        "\n",
        "        for batch_idx, data in enumerate(testloader):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, z_mu, z_logvar, x_recon_mu, x_recon_logvar  = model(data)\n",
        "            loss = customized_loss(x_recon, data, z_mu, z_logvar, x_recon_logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            if epoch % 200 == 0:        \n",
        "                print('====> Epoch: {} Average test loss: {:.4f}'.format(\n",
        "                    epoch, test_loss / len(testloader.dataset)))\n",
        "                \n",
        "            test_losses.append(test_loss / len(testloader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EewLWzctjS5"
      },
      "source": [
        "## Evaluating VAE Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVfEO0nhtrCB"
      },
      "outputs": [],
      "source": [
        "# Comparing the effects of the Reconstructed Values :\n",
        "\n",
        "def compare_results(trainloader, testloader, model, optimizer):\n",
        "\n",
        "  full_recon_x_test = []\n",
        "\n",
        "  # encoding the test data using the trained model's weights\n",
        "  with torch.no_grad():\n",
        "      for batch_idx_recon, data in enumerate(testloader):\n",
        "          data = data.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          recon_batch, z_mu, z_logvar, x_recon_mu, x_recon_logvar = model(data)\n",
        "          full_recon_x_test.append(recon_batch) # adding each of the test batch's data to a full reconstructed batch list\n",
        "\n",
        "  # reshaping all rows of a test batch \n",
        "  recon_batch_reshaped_list = []\n",
        "  recon_row_list = []\n",
        "  int_recon_row_list = []\n",
        "  scaler = testloader.dataset.standardizer\n",
        "\n",
        "  for i in range(len(recon_batch)) :\n",
        "    recon_batch_reshaped_list.append(np.reshape(recon_batch[i].cpu(), (1,-1)))\n",
        "    recon_row_list.append(scaler.inverse_transform(recon_batch_reshaped_list[i].numpy()))\n",
        "    int_recon_row_list.append(np.int_(recon_row_list[i]))\n",
        "\n",
        "\n",
        "  # rescaling the initially normalized and rescaled data\n",
        "  real_reshaped_list = []\n",
        "  real_row_list = []\n",
        "  int_real_row_list = []\n",
        "  scaler = testloader.dataset.standardizer\n",
        "\n",
        "\n",
        "#We want to compare the same batch form the reconstructed and original test data \n",
        "\n",
        "  for batch_idx_real, data in enumerate(testloader):\n",
        "    untouched_data = data.to(device)\n",
        "\n",
        "  for i in range(len(recon_batch)) :\n",
        "    real_reshaped_list.append(np.reshape(untouched_data[i].cpu(), (1,-1))) # x[0] - the first batch or a specific batch? \n",
        "    real_row_list.append(scaler.inverse_transform(real_reshaped_list[i].numpy()))\n",
        "    int_real_row_list.append(np.int_(real_row_list[i]))\n",
        "\n",
        "  reconstructed_batch = int_recon_row_list\n",
        "  real_batch = int_real_row_list\n",
        "\n",
        "  # zipping real with reconstructed row for each item in the test batch\n",
        "  orig_recon_zip = zip(real_batch, reconstructed_batch)\n",
        "  orig_recon = list(orig_recon_zip)\n",
        "\n",
        "\n",
        "  # checking condition that the test data from the real and reconstructed rows comes from the same batch!\n",
        "  if batch_idx_recon == batch_idx_real:\n",
        "    df = pd.DataFrame(data=orig_recon, columns=['Real Row', 'Reconstructed Row'])\n",
        "\n",
        "  else:\n",
        "    df = pd.DataFrame(data=[0], columns=['Not the from same batch'])\n",
        "\n",
        "  \n",
        "  return df, orig_recon_zip, orig_recon, real_batch, reconstructed_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FoHX_-8vOtx"
      },
      "outputs": [],
      "source": [
        "#A function that computes the difference between each element pair (real, reconstructed)\n",
        "\n",
        "def error_diff(zip_real_recon):\n",
        "  error_diff_pairs = []\n",
        "\n",
        "  for i in range(len(zip_real_recon)):\n",
        "    curr_pair = zip_real_recon[i]\n",
        "    curr_real = curr_pair[0][0]\n",
        "    curr_recon = curr_pair[1][0]\n",
        "    new_list = []\n",
        "    error_diff_pairs.append(new_list)\n",
        "\n",
        "    for j in range(len(curr_real)):\n",
        "      val_real = curr_real[j]\n",
        "      val_recon = curr_recon[j]\n",
        "      error_diff_pairs[i].append(abs(val_recon - val_real))\n",
        "\n",
        "  return error_diff_pairs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "VAE_Model_Git_21/07.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1nvqJz6B96Z0a8z+gKb8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}